# -*- coding: utf-8 -*-
"""pandas.jpynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SbTauJL1fnHb1uZkyZ1VmhDGpNmc5GF2
"""

# READING DATA IN PANDAS 

import pandas as pd

# assigning the data to variables
pokemon_file = 'pokemon_data.csv'
demographics_file = 'demographics.xlsx'

# opening the files using necessaary methods for csv and excel, skiprows as the first row of the 
# demographics file is nonsensical 
df_poke = pd.read_csv(pokemon_file)
df_econ = pd.read_excel(demographics_file, skiprows=1)

# printing the head of the two files
# print(df_poke.head())
# print(df_econ.head())

# getting the column names of the two files
# print(df_poke.columns)
# print(df_econ.columns)

# getting the values for the specific column 
# can use bracket notation as shown or dot notation, dot doesn't really work for two word headers though
# can do multiple columns like df_econ[column1, column2, column4] --> pass as a list 
# print(df_econ['Age dependency ratio (% of working-age population)'])

# accessing rows using integer location / iloc
# print(df_poke.iloc[4])
# print(df_poke.iloc[2:5])

# using integer location / iloc to find a specific location:
# looking for venosaur 
# df_poke.iloc[2, 1]

# iterating through each row using iterrows

# for index, row in df_poke.iterrows():
#     print(index, row)

# for index, row in df_poke.iterrows():
#     print(index,row['Name'])

# using loc instead of iloc --> loc using more contextual clues instead of specific integer based 
# getting rows based on a specific condition
# output is far nicer if you do not use a print statement. --> this goes for all calls to show df!!!
# can use multiple conditions

# print(df_poke.loc[df_poke['Type 1'] == 'Fire'])
# df_poke.loc[df_poke['Type 1'] == 'Fire']
# df_econ.loc[df_econ['Labor force, total'] > 4572356]

# describes dataframes, provides all the important summary statistics:

# df_econ.describe()
# df_poke.describe()

# how you should call anytime you want to display the dataframe, no print statement.  
# df_econ
# df_poke

# sorting data using df.sort_values, has an optional argument for (ascending = False/True).  sorts data 
# and alphabetically:
# df_poke.sort_values('Column name', ascending = true / false )
# df_poke.sort_values('Name', ascending=False)
# df_econ.sort_values(['Population, male', 'Population, female'], ascending=[False, True])

# shows dataframe data types:
# df_econ['Population, male'].dtypes

# Changing values to float, as the data was not allowing for ascending / descending 
# df_econ['Population, male'] = df_econ['Population, male'].astype(float)
# df_econ['Population, female'] = df_econ['Population, female'].astype(float)
# sorted_df = df_econ.sort_values(['Population, male', 'Population, female'], ascending=[False, True])
# sorted_df

# MAKING CHANGES TO DATA

# df_econ
 
# lets say i want to add a male / female percent of population:
# can also use iloc to add to columns, this works for addition

# addition
# df_econ['Population, male (% of total population'] = df_econ.iloc[:, [9, 11]].sum(axis=1)
# df_econ

# df_econ['Population, female (% of total population)'] = df_econ['Population, female'] / df_econ['Population, total'] * 100
# df_econ['Population, male (% of total population)'] = df_econ['Population, male'] / df_econ['Population, total'] * 100
# df_econ

# reogranizing the column names:

# you can just do df_econ = df_econ[[column names in order that you want seperated by commas]]
# or you can make use of list(df.columns.values), be aware that hard coding numbers is sketch 
# if data is subject to further change

# deleting columns:

# df_econ = df_econ.drop(df_econ.columns[[-1, -4]], axis=1)
# df_econ

# or

# df_econ = df_econ.drop(df['column to drop])

# exporting the new data
# csv
# index=False is to prevent the exporting to indclude to pandas indexes as the first column
# there appears to be an issue if you import it as one type and try to export it as another

# df_poke.to_csv('newest.csv', index=False)

# # excel
# df_econ.to_excel('economics.xlsx', index=False)

# # txt
# # no delimiter argument for to csv function, have to use sep instead
# df_poke.to_csv('pokemon.txt', index=False, sep='\t')

# filtering using regular expressions and .contains
# import re
# df_poke.loc[df_poke['Type 1'].str.contains('grass|fire', regex=True, flags=re.I)]

# use ~ in place of ! for not including 
# df_poke.loc[~df_poke['Type 1'].str.contains('grass|fire', regex=True, flags=re.I)]

# filtering if you are looking for rows where the value in a specific column contains a certain phrase/letters:
# all the rows where in the name column the phrase 'pi' is included
# df_poke.loc[df_poke['Name'].str.contains('pi[a-z]*', flags=re.I, regex=True)]
# using the carrot ^ to denote that you want pi at the start of the value:
# df_poke.loc[df_poke['Name'].str.contains('^pi[a-z]*', flags=re.I, regex=True)]

# altering file based on conditionals, this changes all fire types to poop types:
# df_poke.loc[df_poke['Type 1'] == 'Fire', 'Type 1'] = 'Poop'
# df_poke

# to rever this:
# df_poke.loc[df_poke['Type 1'] == 'Poop', 'Type 1'] = 'Fire'
# df_poke

# good idea to have a backup plan, as this process can manipulate data to the point where you 
# cannot return to your start point:
# use to csv method
# df_poke.to_csv('backup.csv', index=False)

# converting all fire type pokemon to legendary status:
# df_poke.loc[df_poke['Type 1'] == 'Fire', 'Legendary'] = True
# df_poke.loc[df_poke['Type 1'] == 'Fire']

# reverting back to non-broken dataframe, where legendary isnt messed up:

# df_poke = pd.read_csv('backup.csv')
# df_poke

# messing with multiple columns and having multiple outcomes based on conditions:

# df_poke.loc[df_poke['Defense'] > 100, ['Legendary', 'Name']] = ['True', 'DPOY']
# df_poke.loc[df_poke['Defense'] > 100]

# aggregate statistics using groupby:

# finding average hp and attack of certain pokemon types:

# df_poke.groupby(['Type 1']).mean().sort_values('Defense', ascending=False)
# df_poke.groupby(['Type 1']).sum().sort_values('Defense', ascending=False)

# using a count column like a result variable to get a clean count output:
# df_poke['Count'] = 1
# df_poke.groupby(['Type 1']).count()['Count']

# working with large datasets:
# can specify size of importation using chunksize argument and a for loop when loading file:
# loads in dataframe 5 rows at a time:
# for df in pd.read_csv('backup.csv', chunksize = 5):
#     print(df)


# shrinking the dataset:
# new_df = pd.DataFrame(columns=df.columns)

# for df in pd.read_csv('backup.csv', chunksize =5):
#     result = df.groupby(['Type 1']).count()

# new_df = pd.concat([new_df, result])

# new_df

